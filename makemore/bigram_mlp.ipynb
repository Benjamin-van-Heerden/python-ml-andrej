{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"../names.txt\", \"r\").read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = [\".\"] + list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "stoi = {c: i for i, c in enumerate(alphabet)}\n",
    "itos = {i: c for i, c in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -----> e\n",
      "..e -----> m\n",
      ".em -----> m\n",
      "emm -----> a\n",
      "mma -----> .\n",
      "olivia\n",
      "... -----> o\n",
      "..o -----> l\n",
      ".ol -----> i\n",
      "oli -----> v\n",
      "liv -----> i\n",
      "ivi -----> a\n",
      "via -----> .\n",
      "ava\n",
      "... -----> a\n",
      "..a -----> v\n",
      ".av -----> a\n",
      "ava -----> .\n",
      "isabella\n",
      "... -----> i\n",
      "..i -----> s\n",
      ".is -----> a\n",
      "isa -----> b\n",
      "sab -----> e\n",
      "abe -----> l\n",
      "bel -----> l\n",
      "ell -----> a\n",
      "lla -----> .\n",
      "sophia\n",
      "... -----> s\n",
      "..s -----> o\n",
      ".so -----> p\n",
      "sop -----> h\n",
      "oph -----> i\n",
      "phi -----> a\n",
      "hia -----> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # how many chars do we take to predict the next one\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + \".\":\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(\"\".join(itos[i] for i in context), \"----->\", itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2)) # embedding lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2102,  1.8627])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2102,  1.8627])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0775,  1.1606,  1.0780,  ..., -0.6771, -0.0154, -1.0539],\n",
       "        [-0.4865,  2.5889,  0.9507,  ..., -0.9230, -2.7063,  1.4529],\n",
       "        [-0.6160, -0.6143, -3.1867,  ..., -0.3173, -0.0066, -0.2492],\n",
       "        ...,\n",
       "        [-0.7416,  0.7968,  3.1199,  ..., -5.0320, -4.3367, -3.2557],\n",
       "        [-0.4371, -3.0989,  0.8531,  ...,  0.2176,  0.0625, -1.9680],\n",
       "        [ 0.1765,  3.1662, -0.5326,  ...,  0.5363,  0.4218, -2.6772]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(-1, 6) @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.0356)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-prob[torch.arange(32), Y].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- rewrite and clean up ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # how many chars do we take to predict the next one\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + \".\":\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(\"\".join(itos[i] for i in context), \"----->\", itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10 ** lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step 1000 = 2.3751165866851807\n",
      "loss at step 2000 = 2.374689817428589\n",
      "loss at step 3000 = 2.3738081455230713\n",
      "loss at step 4000 = 2.3745665550231934\n",
      "loss at step 5000 = 2.373636245727539\n",
      "loss at step 6000 = 2.373716115951538\n",
      "loss at step 7000 = 2.3737881183624268\n",
      "loss at step 8000 = 2.374002456665039\n",
      "loss at step 9000 = 2.373725652694702\n",
      "loss at step 10000 = 2.373215436935425\n"
     ]
    }
   ],
   "source": [
    "# lri = []\n",
    "# lossi = []\n",
    "for i in range(10000):\n",
    "    batch_ixs = torch.randint(0, X.shape[0], (32,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[batch_ixs]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdim=True)\n",
    "    # loss = -prob[torch.arange(emb.shape[0]), Y].log().mean()\n",
    "    reg = 0.05 * sum((p ** 2).mean() for p in parameters)\n",
    "    loss = F.cross_entropy(logits, Y[batch_ixs]) + reg\n",
    "    \n",
    "    if (i + 1) % 1000 == 0:\n",
    "        emb = C[X]\n",
    "        h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        overall_loss = F.cross_entropy(logits, Y)\n",
    "        print(f\"loss at step {i+1} = {overall_loss.item()}\")\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    # lossi.append(loss.item())\n",
    "    loss.backward()\n",
    "    # lr = lrs[i]\n",
    "    lr = 0.002\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    # lri.append(lr)\n",
    "        \n",
    "# base model had loss of 2.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training split, dev/validation split, test split\n",
    "# train params, train hyperparams, evaluate (should be done infrequently)\n",
    "# 80, 10, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    random.seed(42)\n",
    "    random.shuffle(words)\n",
    "    \n",
    "    block_size = 3\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    n1 = int(0.8 * len(words))\n",
    "    n2 = int(0.9 * len(words))\n",
    "\n",
    "    Xtr, Ytr = X[:n1], Y[:n1]\n",
    "    Xdev, Ydev = X[n1:n2], Y[n1:n2]\n",
    "    Xte, Yte = X[n2:], Y[n2:]\n",
    "\n",
    "    return Xtr, Ytr, Xdev, Ydev, Xte, Yte\n",
    "\n",
    "Xtr, Ytr, Xdev, Ydev, Xte, Yte = build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 300), generator=g)\n",
    "b1 = torch.randn(300, generator=g)\n",
    "W2 = torch.randn((300, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10281"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10 ** lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lri = []\n",
    "# lossi = []\n",
    "# stepi = []\n",
    "\n",
    "for i in range(30000):\n",
    "    batch_ixs = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[batch_ixs]]\n",
    "    h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    # reg = 0.05 * sum((p ** 2).mean() for p in parameters)\n",
    "    loss = F.cross_entropy(logits, Ytr[batch_ixs])\n",
    "\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "    # alpha = 0.175\n",
    "    # gamma = 1\n",
    "    # beta = -np.log(gamma) - 1\n",
    "    # lr = np.exp(-((i + 1) ** alpha + beta))\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    # stepi.append(i)\n",
    "    # lri.append(lr)\n",
    "    # lossi.append(loss.item())\n",
    "\n",
    "    # if (i + 1) % 1000 == 0:\n",
    "    #     emb = C[Xtr]\n",
    "    #     h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "    #     logits = h @ W2 + b2\n",
    "    #     overall_loss = F.cross_entropy(logits, Ytr)\n",
    "    #     print(f\"tr loss at step {i+1} = {overall_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr loss at step 30000 = 2.2670202255249023\n"
     ]
    }
   ],
   "source": [
    "emb = C[Xtr]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "overall_loss = F.cross_entropy(logits, Ytr)\n",
    "print(f\"tr loss at step {i+1} = {overall_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev loss at step 30000 = 2.361804485321045\n"
     ]
    }
   ],
   "source": [
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1, 6) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "overall_loss = F.cross_entropy(logits, Ydev)\n",
    "print(f\"dev loss at step {i+1} = {overall_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(C[:, 0].data, C[:, 1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\")\n",
    "plt.grid(\"minor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale up to 10d embeddings\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "W1 = torch.randn((30, 200), generator=g)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10 ** lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "tr loss at step 1000 = 3.537635087966919\n",
      "dev loss = 3.7564518451690674\n",
      "lr = 0.21417267312607005\n",
      "--------------------\n",
      "tr loss at step 2000 = 2.9848837852478027\n",
      "dev loss = 3.198927879333496\n",
      "lr = 0.16692279681244065\n",
      "--------------------\n",
      "tr loss at step 3000 = 2.716989755630493\n",
      "dev loss = 2.958570957183838\n",
      "lr = 0.14267129234635761\n",
      "--------------------\n",
      "tr loss at step 4000 = 2.5912625789642334\n",
      "dev loss = 2.8814518451690674\n",
      "lr = 0.1269547216796825\n",
      "--------------------\n",
      "tr loss at step 5000 = 2.494887590408325\n",
      "dev loss = 2.7660045623779297\n",
      "lr = 0.11559900858694484\n",
      "--------------------\n",
      "tr loss at step 6000 = 2.4630424976348877\n",
      "dev loss = 2.7306017875671387\n",
      "lr = 0.10685182016761786\n",
      "--------------------\n",
      "tr loss at step 7000 = 2.392534017562866\n",
      "dev loss = 2.673924207687378\n",
      "lr = 0.09982199903217022\n",
      "--------------------\n",
      "tr loss at step 8000 = 2.438981533050537\n",
      "dev loss = 2.710045099258423\n",
      "lr = 0.09399875128192368\n",
      "--------------------\n",
      "tr loss at step 9000 = 2.3353307247161865\n",
      "dev loss = 2.59871768951416\n",
      "lr = 0.08906410479836022\n",
      "--------------------\n",
      "tr loss at step 10000 = 2.339268684387207\n",
      "dev loss = 2.6115996837615967\n",
      "lr = 0.08480773679759514\n",
      "--------------------\n",
      "tr loss at step 11000 = 2.385930299758911\n",
      "dev loss = 2.66219162940979\n",
      "lr = 0.08108389198616167\n",
      "--------------------\n",
      "tr loss at step 12000 = 2.3061790466308594\n",
      "dev loss = 2.5973682403564453\n",
      "lr = 0.07778775595152827\n",
      "--------------------\n",
      "tr loss at step 13000 = 2.2774815559387207\n",
      "dev loss = 2.5631909370422363\n",
      "lr = 0.07484165349438816\n",
      "--------------------\n",
      "tr loss at step 14000 = 2.377432346343994\n",
      "dev loss = 2.6527082920074463\n",
      "lr = 0.07218656613137875\n",
      "--------------------\n",
      "tr loss at step 15000 = 2.250157594680786\n",
      "dev loss = 2.538755416870117\n",
      "lr = 0.06977669612934174\n",
      "--------------------\n",
      "tr loss at step 16000 = 2.23669171333313\n",
      "dev loss = 2.529473304748535\n",
      "lr = 0.06757585859244965\n",
      "--------------------\n",
      "tr loss at step 17000 = 2.2452280521392822\n",
      "dev loss = 2.528496503829956\n",
      "lr = 0.06555501431506558\n",
      "--------------------\n",
      "tr loss at step 18000 = 2.2353899478912354\n",
      "dev loss = 2.513671875\n",
      "lr = 0.06369053875279415\n",
      "--------------------\n",
      "tr loss at step 19000 = 2.226212739944458\n",
      "dev loss = 2.534013509750366\n",
      "lr = 0.06196297995491983\n",
      "--------------------\n",
      "tr loss at step 20000 = 2.196767568588257\n",
      "dev loss = 2.4887139797210693\n",
      "lr = 0.06035614960821989\n",
      "--------------------\n",
      "tr loss at step 21000 = 2.190473794937134\n",
      "dev loss = 2.490616798400879\n",
      "lr = 0.05885644613572952\n",
      "--------------------\n",
      "tr loss at step 22000 = 2.221611738204956\n",
      "dev loss = 2.502103328704834\n",
      "lr = 0.05745234268697393\n",
      "--------------------\n",
      "tr loss at step 23000 = 2.189685344696045\n",
      "dev loss = 2.495434522628784\n",
      "lr = 0.056133994390743513\n",
      "--------------------\n",
      "tr loss at step 24000 = 2.1781952381134033\n",
      "dev loss = 2.4839587211608887\n",
      "lr = 0.05489293325521601\n",
      "--------------------\n",
      "tr loss at step 25000 = 2.206511974334717\n",
      "dev loss = 2.5048470497131348\n",
      "lr = 0.05372182841776908\n",
      "--------------------\n",
      "tr loss at step 26000 = 2.2044429779052734\n",
      "dev loss = 2.513683557510376\n",
      "lr = 0.05261429576330475\n",
      "--------------------\n",
      "tr loss at step 27000 = 2.1906962394714355\n",
      "dev loss = 2.5079593658447266\n",
      "lr = 0.051564745288011256\n",
      "--------------------\n",
      "tr loss at step 28000 = 2.1628670692443848\n",
      "dev loss = 2.478703737258911\n",
      "lr = 0.05056825764108792\n",
      "--------------------\n",
      "tr loss at step 29000 = 2.1516451835632324\n",
      "dev loss = 2.464738130569458\n",
      "lr = 0.04962048345107984\n",
      "--------------------\n",
      "tr loss at step 30000 = 2.1667165756225586\n",
      "dev loss = 2.4803175926208496\n",
      "lr = 0.04871756061146952\n",
      "--------------------\n",
      "tr loss at step 31000 = 2.1622605323791504\n",
      "dev loss = 2.471778392791748\n",
      "lr = 0.04785604584522639\n",
      "--------------------\n",
      "tr loss at step 32000 = 2.163851022720337\n",
      "dev loss = 2.484745740890503\n",
      "lr = 0.04703285771392396\n",
      "--------------------\n",
      "tr loss at step 33000 = 2.1466050148010254\n",
      "dev loss = 2.454880714416504\n",
      "lr = 0.046245228868682875\n",
      "--------------------\n",
      "tr loss at step 34000 = 2.141521692276001\n",
      "dev loss = 2.4713709354400635\n",
      "lr = 0.04549066581658872\n",
      "--------------------\n",
      "tr loss at step 35000 = 2.1334471702575684\n",
      "dev loss = 2.454402446746826\n",
      "lr = 0.04476691483888968\n",
      "--------------------\n",
      "tr loss at step 36000 = 2.123220682144165\n",
      "dev loss = 2.4482638835906982\n",
      "lr = 0.04407193297577026\n",
      "--------------------\n",
      "tr loss at step 37000 = 2.134777545928955\n",
      "dev loss = 2.448499917984009\n",
      "lr = 0.043403863208108386\n",
      "--------------------\n",
      "tr loss at step 38000 = 2.126903533935547\n",
      "dev loss = 2.442126750946045\n",
      "lr = 0.04276101313484154\n",
      "--------------------\n",
      "tr loss at step 39000 = 2.136995315551758\n",
      "dev loss = 2.449185609817505\n",
      "lr = 0.04214183657675691\n",
      "--------------------\n",
      "tr loss at step 40000 = 2.1371428966522217\n",
      "dev loss = 2.4689202308654785\n",
      "lr = 0.04154491764211115\n",
      "--------------------\n",
      "tr loss at step 41000 = 2.1538639068603516\n",
      "dev loss = 2.4884352684020996\n",
      "lr = 0.04096895687277205\n",
      "--------------------\n",
      "tr loss at step 42000 = 2.14162278175354\n",
      "dev loss = 2.46699595451355\n",
      "lr = 0.04041275915629969\n",
      "--------------------\n",
      "tr loss at step 43000 = 2.1199612617492676\n",
      "dev loss = 2.46077823638916\n",
      "lr = 0.039875223143155336\n",
      "--------------------\n",
      "tr loss at step 44000 = 2.1398518085479736\n",
      "dev loss = 2.4829821586608887\n",
      "lr = 0.03935533195179015\n",
      "--------------------\n",
      "tr loss at step 45000 = 2.1142399311065674\n",
      "dev loss = 2.443248748779297\n",
      "lr = 0.0388521449798469\n",
      "--------------------\n",
      "tr loss at step 46000 = 2.1188724040985107\n",
      "dev loss = 2.4617788791656494\n",
      "lr = 0.038364790668751106\n",
      "--------------------\n",
      "tr loss at step 47000 = 2.112990140914917\n",
      "dev loss = 2.4511446952819824\n",
      "lr = 0.03789246009284751\n",
      "--------------------\n",
      "tr loss at step 48000 = 2.1048367023468018\n",
      "dev loss = 2.44441819190979\n",
      "lr = 0.03743440126396894\n",
      "--------------------\n",
      "tr loss at step 49000 = 2.115485668182373\n",
      "dev loss = 2.453632116317749\n",
      "lr = 0.0369899140586884\n",
      "--------------------\n",
      "tr loss at step 50000 = 2.106982707977295\n",
      "dev loss = 2.4450879096984863\n",
      "lr = 0.0365583456891419\n",
      "--------------------\n",
      "tr loss at step 51000 = 2.0974888801574707\n",
      "dev loss = 2.444822311401367\n",
      "lr = 0.03613908664970889\n",
      "--------------------\n",
      "tr loss at step 52000 = 2.091761589050293\n",
      "dev loss = 2.433110237121582\n",
      "lr = 0.03573156708141002\n",
      "--------------------\n",
      "tr loss at step 53000 = 2.0992565155029297\n",
      "dev loss = 2.4369518756866455\n",
      "lr = 0.03533525350394519\n",
      "--------------------\n",
      "tr loss at step 54000 = 2.1034302711486816\n",
      "dev loss = 2.4436020851135254\n",
      "lr = 0.03494964587211076\n",
      "--------------------\n",
      "tr loss at step 55000 = 2.1171960830688477\n",
      "dev loss = 2.466980218887329\n",
      "lr = 0.03457427491911812\n",
      "--------------------\n",
      "tr loss at step 56000 = 2.095714807510376\n",
      "dev loss = 2.428398609161377\n",
      "lr = 0.03420869975425435\n",
      "--------------------\n",
      "tr loss at step 57000 = 2.088057279586792\n",
      "dev loss = 2.4327361583709717\n",
      "lr = 0.033852505686526285\n",
      "--------------------\n",
      "tr loss at step 58000 = 2.090956449508667\n",
      "dev loss = 2.441131353378296\n",
      "lr = 0.03350530224952296\n",
      "--------------------\n",
      "tr loss at step 59000 = 2.0952186584472656\n",
      "dev loss = 2.4447286128997803\n",
      "lr = 0.03316672140581786\n",
      "--------------------\n",
      "tr loss at step 60000 = 2.094285726547241\n",
      "dev loss = 2.4424116611480713\n",
      "lr = 0.0328364159118894\n",
      "--------------------\n",
      "tr loss at step 61000 = 2.0858070850372314\n",
      "dev loss = 2.439800500869751\n",
      "lr = 0.032514057826829666\n",
      "--------------------\n",
      "tr loss at step 62000 = 2.0995936393737793\n",
      "dev loss = 2.4585654735565186\n",
      "lr = 0.03219933715009723\n",
      "--------------------\n",
      "tr loss at step 63000 = 2.0836126804351807\n",
      "dev loss = 2.4273595809936523\n",
      "lr = 0.03189196057528946\n",
      "--------------------\n",
      "tr loss at step 64000 = 2.0767178535461426\n",
      "dev loss = 2.4273173809051514\n",
      "lr = 0.0315916503484073\n",
      "--------------------\n",
      "tr loss at step 65000 = 2.0859928131103516\n",
      "dev loss = 2.4432425498962402\n",
      "lr = 0.03129814322038889\n",
      "--------------------\n",
      "tr loss at step 66000 = 2.079165458679199\n",
      "dev loss = 2.436619281768799\n",
      "lr = 0.03101118948482869\n",
      "--------------------\n",
      "tr loss at step 67000 = 2.0773441791534424\n",
      "dev loss = 2.422912120819092\n",
      "lr = 0.030730552092794704\n",
      "--------------------\n",
      "tr loss at step 68000 = 2.0904462337493896\n",
      "dev loss = 2.4511921405792236\n",
      "lr = 0.030456005837532427\n",
      "--------------------\n",
      "tr loss at step 69000 = 2.0707554817199707\n",
      "dev loss = 2.4288811683654785\n",
      "lr = 0.03018733660261177\n",
      "--------------------\n",
      "tr loss at step 70000 = 2.063852310180664\n",
      "dev loss = 2.435321807861328\n",
      "lr = 0.029924340667751632\n",
      "--------------------\n",
      "tr loss at step 71000 = 2.0675880908966064\n",
      "dev loss = 2.433199167251587\n",
      "lr = 0.0296668240671527\n",
      "--------------------\n",
      "tr loss at step 72000 = 2.063340902328491\n",
      "dev loss = 2.429577350616455\n",
      "lr = 0.029414601995698897\n",
      "--------------------\n",
      "tr loss at step 73000 = 2.075723171234131\n",
      "dev loss = 2.4439003467559814\n",
      "lr = 0.029167498258854185\n",
      "--------------------\n",
      "tr loss at step 74000 = 2.0685439109802246\n",
      "dev loss = 2.4297473430633545\n",
      "lr = 0.028925344762496953\n",
      "--------------------\n",
      "tr loss at step 75000 = 2.0775582790374756\n",
      "dev loss = 2.4408442974090576\n",
      "lr = 0.028687981039303184\n",
      "--------------------\n",
      "tr loss at step 76000 = 2.0623562335968018\n",
      "dev loss = 2.4156086444854736\n",
      "lr = 0.028455253808617104\n",
      "--------------------\n",
      "tr loss at step 77000 = 2.06784725189209\n",
      "dev loss = 2.4232914447784424\n",
      "lr = 0.028227016567040943\n",
      "--------------------\n",
      "tr loss at step 78000 = 2.054276704788208\n",
      "dev loss = 2.4164977073669434\n",
      "lr = 0.02800312920723684\n",
      "--------------------\n",
      "tr loss at step 79000 = 2.064216136932373\n",
      "dev loss = 2.4335317611694336\n",
      "lr = 0.027783457662666943\n",
      "--------------------\n",
      "tr loss at step 80000 = 2.052173137664795\n",
      "dev loss = 2.419232130050659\n",
      "lr = 0.027567873576207467\n",
      "--------------------\n",
      "tr loss at step 81000 = 2.052814245223999\n",
      "dev loss = 2.410681962966919\n",
      "lr = 0.027356253990760075\n",
      "--------------------\n",
      "tr loss at step 82000 = 2.0469980239868164\n",
      "dev loss = 2.406257390975952\n",
      "lr = 0.0271484810601517\n",
      "--------------------\n",
      "tr loss at step 83000 = 2.044877767562866\n",
      "dev loss = 2.402811288833618\n",
      "lr = 0.02694444177876685\n",
      "--------------------\n",
      "tr loss at step 84000 = 2.041426181793213\n",
      "dev loss = 2.4092938899993896\n",
      "lr = 0.02674402772849101\n",
      "--------------------\n",
      "tr loss at step 85000 = 2.050959825515747\n",
      "dev loss = 2.415658712387085\n",
      "lr = 0.02654713484166904\n",
      "--------------------\n",
      "tr loss at step 86000 = 2.0564165115356445\n",
      "dev loss = 2.4203803539276123\n",
      "lr = 0.026353663178891075\n",
      "--------------------\n",
      "tr loss at step 87000 = 2.0497560501098633\n",
      "dev loss = 2.4244303703308105\n",
      "lr = 0.026163516720521144\n",
      "--------------------\n",
      "tr loss at step 88000 = 2.0461816787719727\n",
      "dev loss = 2.4227607250213623\n",
      "lr = 0.025976603170972945\n",
      "--------------------\n",
      "tr loss at step 89000 = 2.052175283432007\n",
      "dev loss = 2.4307477474212646\n",
      "lr = 0.025792833774820313\n",
      "--------------------\n",
      "tr loss at step 90000 = 2.0547735691070557\n",
      "dev loss = 2.43039870262146\n",
      "lr = 0.025612123143904448\n",
      "--------------------\n",
      "tr loss at step 91000 = 2.040440559387207\n",
      "dev loss = 2.419044256210327\n",
      "lr = 0.02543438909466771\n",
      "--------------------\n",
      "tr loss at step 92000 = 2.052680015563965\n",
      "dev loss = 2.434185028076172\n",
      "lr = 0.02525955249500577\n",
      "--------------------\n",
      "tr loss at step 93000 = 2.03913950920105\n",
      "dev loss = 2.4238216876983643\n",
      "lr = 0.02508753711998526\n",
      "--------------------\n",
      "tr loss at step 94000 = 2.0463974475860596\n",
      "dev loss = 2.4336960315704346\n",
      "lr = 0.02491826951582638\n",
      "--------------------\n",
      "tr loss at step 95000 = 2.0373268127441406\n",
      "dev loss = 2.411992311477661\n",
      "lr = 0.024751678871595132\n",
      "--------------------\n",
      "tr loss at step 96000 = 2.0373387336730957\n",
      "dev loss = 2.409733295440674\n",
      "lr = 0.024587696898093852\n",
      "--------------------\n",
      "tr loss at step 97000 = 2.0511491298675537\n",
      "dev loss = 2.441044569015503\n",
      "lr = 0.024426257713476396\n",
      "--------------------\n",
      "tr loss at step 98000 = 2.0380685329437256\n",
      "dev loss = 2.425477981567383\n",
      "lr = 0.024267297735150652\n",
      "--------------------\n",
      "tr loss at step 99000 = 2.0377326011657715\n",
      "dev loss = 2.419562578201294\n",
      "lr = 0.024110755577563356\n",
      "--------------------\n",
      "tr loss at step 100000 = 2.0382726192474365\n",
      "dev loss = 2.43017315864563\n",
      "lr = 0.023956571955491994\n",
      "--------------------\n",
      "tr loss at step 101000 = 2.030565023422241\n",
      "dev loss = 2.4072203636169434\n",
      "lr = 0.023804689592496184\n",
      "--------------------\n",
      "tr loss at step 102000 = 2.034104824066162\n",
      "dev loss = 2.417999029159546\n",
      "lr = 0.023655053134205704\n",
      "--------------------\n",
      "tr loss at step 103000 = 2.0314950942993164\n",
      "dev loss = 2.415961742401123\n",
      "lr = 0.023507609066145803\n",
      "--------------------\n",
      "tr loss at step 104000 = 2.028097629547119\n",
      "dev loss = 2.4127442836761475\n",
      "lr = 0.023362305635821277\n",
      "--------------------\n",
      "tr loss at step 105000 = 2.024714231491089\n",
      "dev loss = 2.40486741065979\n",
      "lr = 0.023219092778800694\n",
      "--------------------\n",
      "tr loss at step 106000 = 2.0297670364379883\n",
      "dev loss = 2.4176182746887207\n",
      "lr = 0.02307792204856006\n",
      "--------------------\n",
      "tr loss at step 107000 = 2.0378143787384033\n",
      "dev loss = 2.4259378910064697\n",
      "lr = 0.022938746549861326\n",
      "--------------------\n",
      "tr loss at step 108000 = 2.037076950073242\n",
      "dev loss = 2.4193222522735596\n",
      "lr = 0.0228015208754575\n",
      "--------------------\n",
      "tr loss at step 109000 = 2.0268402099609375\n",
      "dev loss = 2.41678524017334\n",
      "lr = 0.022666201045928917\n",
      "--------------------\n",
      "tr loss at step 110000 = 2.0219171047210693\n",
      "dev loss = 2.4093873500823975\n",
      "lr = 0.022532744452469607\n",
      "--------------------\n",
      "tr loss at step 111000 = 2.0279829502105713\n",
      "dev loss = 2.4078330993652344\n",
      "lr = 0.022401109802453537\n",
      "--------------------\n",
      "tr loss at step 112000 = 2.0248148441314697\n",
      "dev loss = 2.41933274269104\n",
      "lr = 0.022271257067622924\n",
      "--------------------\n",
      "tr loss at step 113000 = 2.0306272506713867\n",
      "dev loss = 2.4281420707702637\n",
      "lr = 0.022143147434749814\n",
      "--------------------\n",
      "tr loss at step 114000 = 2.0343053340911865\n",
      "dev loss = 2.423306465148926\n",
      "lr = 0.02201674325863292\n",
      "--------------------\n",
      "tr loss at step 115000 = 2.0184671878814697\n",
      "dev loss = 2.409947633743286\n",
      "lr = 0.021892008017299916\n",
      "--------------------\n",
      "tr loss at step 116000 = 2.0214803218841553\n",
      "dev loss = 2.4108948707580566\n",
      "lr = 0.02176890626929367\n",
      "--------------------\n",
      "tr loss at step 117000 = 2.027703046798706\n",
      "dev loss = 2.4299187660217285\n",
      "lr = 0.02164740361292877\n",
      "--------------------\n",
      "tr loss at step 118000 = 2.0159480571746826\n",
      "dev loss = 2.4066739082336426\n",
      "lr = 0.02152746664741175\n",
      "--------------------\n",
      "tr loss at step 119000 = 2.02701473236084\n",
      "dev loss = 2.413724422454834\n",
      "lr = 0.021409062935725004\n",
      "--------------------\n",
      "tr loss at step 120000 = 2.0154333114624023\n",
      "dev loss = 2.411588191986084\n",
      "lr = 0.021292160969180223\n",
      "--------------------\n",
      "tr loss at step 121000 = 2.014502763748169\n",
      "dev loss = 2.406801700592041\n",
      "lr = 0.021176730133553753\n",
      "--------------------\n",
      "tr loss at step 122000 = 2.0334455966949463\n",
      "dev loss = 2.423604726791382\n",
      "lr = 0.021062740676720523\n",
      "--------------------\n",
      "tr loss at step 123000 = 2.02074933052063\n",
      "dev loss = 2.42421817779541\n",
      "lr = 0.020950163677708966\n",
      "--------------------\n",
      "tr loss at step 124000 = 2.0156986713409424\n",
      "dev loss = 2.419785261154175\n",
      "lr = 0.020838971017103733\n",
      "--------------------\n",
      "tr loss at step 125000 = 2.0133183002471924\n",
      "dev loss = 2.4105312824249268\n",
      "lr = 0.020729135348727234\n",
      "--------------------\n",
      "tr loss at step 126000 = 2.013991117477417\n",
      "dev loss = 2.412429094314575\n",
      "lr = 0.02062063007253509\n",
      "--------------------\n",
      "tr loss at step 127000 = 2.013425350189209\n",
      "dev loss = 2.406367063522339\n",
      "lr = 0.02051342930866453\n",
      "--------------------\n",
      "tr loss at step 128000 = 2.016679286956787\n",
      "dev loss = 2.4259402751922607\n",
      "lr = 0.02040750787257821\n",
      "--------------------\n",
      "tr loss at step 129000 = 2.0059237480163574\n",
      "dev loss = 2.3973307609558105\n",
      "lr = 0.02030284125124889\n",
      "--------------------\n",
      "tr loss at step 130000 = 2.020113229751587\n",
      "dev loss = 2.4216928482055664\n",
      "lr = 0.020199405580334256\n",
      "--------------------\n",
      "tr loss at step 131000 = 2.0131516456604004\n",
      "dev loss = 2.422869920730591\n",
      "lr = 0.020097177622293075\n",
      "--------------------\n",
      "tr loss at step 132000 = 2.011554718017578\n",
      "dev loss = 2.4092328548431396\n",
      "lr = 0.019996134745397545\n",
      "--------------------\n",
      "tr loss at step 133000 = 2.0098206996917725\n",
      "dev loss = 2.4115395545959473\n",
      "lr = 0.01989625490359835\n",
      "--------------------\n",
      "tr loss at step 134000 = 2.0178208351135254\n",
      "dev loss = 2.4290146827697754\n",
      "lr = 0.019797516617202054\n",
      "--------------------\n",
      "tr loss at step 135000 = 2.0100502967834473\n",
      "dev loss = 2.4145424365997314\n",
      "lr = 0.019699898954322236\n",
      "--------------------\n",
      "tr loss at step 136000 = 2.016115188598633\n",
      "dev loss = 2.426513195037842\n",
      "lr = 0.019603381513067952\n",
      "--------------------\n",
      "tr loss at step 137000 = 2.0010085105895996\n",
      "dev loss = 2.4041473865509033\n",
      "lr = 0.019507944404435196\n",
      "--------------------\n",
      "tr loss at step 138000 = 2.000894546508789\n",
      "dev loss = 2.413123369216919\n",
      "lr = 0.019413568235868584\n",
      "--------------------\n",
      "tr loss at step 139000 = 2.0118157863616943\n",
      "dev loss = 2.4211392402648926\n",
      "lr = 0.019320234095462793\n",
      "--------------------\n",
      "tr loss at step 140000 = 2.0047101974487305\n",
      "dev loss = 2.412829875946045\n",
      "lr = 0.019227923536774102\n",
      "--------------------\n",
      "tr loss at step 141000 = 2.010009288787842\n",
      "dev loss = 2.42718243598938\n",
      "lr = 0.019136618564214662\n",
      "--------------------\n",
      "tr loss at step 142000 = 1.999042272567749\n",
      "dev loss = 2.406444787979126\n",
      "lr = 0.01904630161900309\n",
      "--------------------\n",
      "tr loss at step 143000 = 2.0073421001434326\n",
      "dev loss = 2.4180750846862793\n",
      "lr = 0.018956955565646576\n",
      "--------------------\n",
      "tr loss at step 144000 = 2.0072336196899414\n",
      "dev loss = 2.4216623306274414\n",
      "lr = 0.01886856367893083\n",
      "--------------------\n",
      "tr loss at step 145000 = 2.0044727325439453\n",
      "dev loss = 2.4173717498779297\n",
      "lr = 0.018781109631395464\n",
      "--------------------\n",
      "tr loss at step 146000 = 1.9987114667892456\n",
      "dev loss = 2.4043853282928467\n",
      "lr = 0.01869457748127364\n",
      "--------------------\n",
      "tr loss at step 147000 = 2.0043396949768066\n",
      "dev loss = 2.413414239883423\n",
      "lr = 0.018608951660875568\n",
      "--------------------\n",
      "tr loss at step 148000 = 2.005178451538086\n",
      "dev loss = 2.415886163711548\n",
      "lr = 0.01852421696539691\n",
      "--------------------\n",
      "tr loss at step 149000 = 1.9962763786315918\n",
      "dev loss = 2.4193103313446045\n",
      "lr = 0.018440358542133606\n",
      "--------------------\n",
      "tr loss at step 150000 = 2.0016098022460938\n",
      "dev loss = 2.414469003677368\n",
      "lr = 0.01835736188008592\n",
      "--------------------\n",
      "tr loss at step 151000 = 1.9939321279525757\n",
      "dev loss = 2.406404495239258\n",
      "lr = 0.018275212799935176\n",
      "--------------------\n",
      "tr loss at step 152000 = 1.9974435567855835\n",
      "dev loss = 2.412446975708008\n",
      "lr = 0.018193897444377563\n",
      "--------------------\n",
      "tr loss at step 153000 = 1.9999703168869019\n",
      "dev loss = 2.4131124019622803\n",
      "lr = 0.018113402268799785\n",
      "--------------------\n",
      "tr loss at step 154000 = 2.0044119358062744\n",
      "dev loss = 2.4122533798217773\n",
      "lr = 0.01803371403228275\n",
      "--------------------\n",
      "tr loss at step 155000 = 1.997231364250183\n",
      "dev loss = 2.419379234313965\n",
      "lr = 0.017954819788919486\n",
      "--------------------\n",
      "tr loss at step 156000 = 1.998995065689087\n",
      "dev loss = 2.4125847816467285\n",
      "lr = 0.017876706879434403\n",
      "--------------------\n",
      "tr loss at step 157000 = 1.9972745180130005\n",
      "dev loss = 2.4086320400238037\n",
      "lr = 0.017799362923091665\n",
      "--------------------\n",
      "tr loss at step 158000 = 1.9963252544403076\n",
      "dev loss = 2.427039861679077\n",
      "lr = 0.017722775809881054\n",
      "--------------------\n",
      "tr loss at step 159000 = 2.0014874935150146\n",
      "dev loss = 2.4201812744140625\n",
      "lr = 0.01764693369296986\n",
      "--------------------\n",
      "tr loss at step 160000 = 1.992362380027771\n",
      "dev loss = 2.41764760017395\n",
      "lr = 0.017571824981410467\n",
      "--------------------\n",
      "tr loss at step 161000 = 1.9930249452590942\n",
      "dev loss = 2.4143476486206055\n",
      "lr = 0.017497438333093297\n",
      "--------------------\n",
      "tr loss at step 162000 = 1.995237112045288\n",
      "dev loss = 2.4109697341918945\n",
      "lr = 0.01742376264793537\n",
      "--------------------\n",
      "tr loss at step 163000 = 1.9927152395248413\n",
      "dev loss = 2.415147542953491\n",
      "lr = 0.017350787061295395\n",
      "--------------------\n",
      "tr loss at step 164000 = 1.9921563863754272\n",
      "dev loss = 2.4106833934783936\n",
      "lr = 0.01727850093760635\n",
      "--------------------\n",
      "tr loss at step 165000 = 1.9971171617507935\n",
      "dev loss = 2.4265384674072266\n",
      "lr = 0.01720689386421717\n",
      "--------------------\n",
      "tr loss at step 166000 = 1.9849762916564941\n",
      "dev loss = 2.4125397205352783\n",
      "lr = 0.01713595564543569\n",
      "--------------------\n",
      "tr loss at step 167000 = 1.996786117553711\n",
      "dev loss = 2.4121901988983154\n",
      "lr = 0.017065676296764642\n",
      "--------------------\n",
      "tr loss at step 168000 = 1.986755132675171\n",
      "dev loss = 2.407632350921631\n",
      "lr = 0.016996046039323847\n",
      "--------------------\n",
      "tr loss at step 169000 = 1.9911044836044312\n",
      "dev loss = 2.4191577434539795\n",
      "lr = 0.016927055294451227\n",
      "--------------------\n",
      "tr loss at step 170000 = 1.9946372509002686\n",
      "dev loss = 2.4193875789642334\n",
      "lr = 0.016858694678475995\n",
      "--------------------\n",
      "tr loss at step 171000 = 1.9876919984817505\n",
      "dev loss = 2.4070844650268555\n",
      "lr = 0.016790954997657613\n",
      "--------------------\n",
      "tr loss at step 172000 = 1.9950278997421265\n",
      "dev loss = 2.419360637664795\n",
      "lr = 0.01672382724328427\n",
      "--------------------\n",
      "tr loss at step 173000 = 1.9849979877471924\n",
      "dev loss = 2.4153034687042236\n",
      "lr = 0.0166573025869249\n",
      "--------------------\n",
      "tr loss at step 174000 = 1.9935215711593628\n",
      "dev loss = 2.4099366664886475\n",
      "lr = 0.01659137237582936\n",
      "--------------------\n",
      "tr loss at step 175000 = 1.9906182289123535\n",
      "dev loss = 2.4218573570251465\n",
      "lr = 0.016526028128471055\n",
      "--------------------\n",
      "tr loss at step 176000 = 1.9836524724960327\n",
      "dev loss = 2.4142792224884033\n",
      "lr = 0.016461261530226864\n",
      "--------------------\n",
      "tr loss at step 177000 = 1.9876149892807007\n",
      "dev loss = 2.4223520755767822\n",
      "lr = 0.01639706442918958\n",
      "--------------------\n",
      "tr loss at step 178000 = 1.9873569011688232\n",
      "dev loss = 2.426971435546875\n",
      "lr = 0.016333428832107897\n",
      "--------------------\n",
      "tr loss at step 179000 = 1.9862960577011108\n",
      "dev loss = 2.4175541400909424\n",
      "lr = 0.01627034690044942\n",
      "--------------------\n",
      "tr loss at step 180000 = 1.982211947441101\n",
      "dev loss = 2.414332389831543\n",
      "lr = 0.016207810946582444\n",
      "--------------------\n",
      "tr loss at step 181000 = 1.9884908199310303\n",
      "dev loss = 2.4250574111938477\n",
      "lr = 0.01614581343007206\n",
      "--------------------\n",
      "tr loss at step 182000 = 1.9887382984161377\n",
      "dev loss = 2.4132776260375977\n",
      "lr = 0.016084346954086746\n",
      "--------------------\n",
      "tr loss at step 183000 = 1.9886226654052734\n",
      "dev loss = 2.4275197982788086\n",
      "lr = 0.016023404261911536\n",
      "--------------------\n",
      "tr loss at step 184000 = 1.9819608926773071\n",
      "dev loss = 2.4121885299682617\n",
      "lr = 0.015962978233564032\n",
      "--------------------\n",
      "tr loss at step 185000 = 1.979986548423767\n",
      "dev loss = 2.4124093055725098\n",
      "lr = 0.015903061882509596\n",
      "--------------------\n",
      "tr loss at step 186000 = 1.9775129556655884\n",
      "dev loss = 2.41347336769104\n",
      "lr = 0.01584364835247255\n",
      "--------------------\n",
      "tr loss at step 187000 = 1.984908938407898\n",
      "dev loss = 2.4172186851501465\n",
      "lr = 0.015784730914339785\n",
      "--------------------\n",
      "tr loss at step 188000 = 1.9827032089233398\n",
      "dev loss = 2.423691749572754\n",
      "lr = 0.01572630296315387\n",
      "--------------------\n",
      "tr loss at step 189000 = 1.9803204536437988\n",
      "dev loss = 2.428668975830078\n",
      "lr = 0.015668358015192545\n",
      "--------------------\n",
      "tr loss at step 190000 = 1.985717535018921\n",
      "dev loss = 2.4296720027923584\n",
      "lr = 0.015610889705131555\n",
      "--------------------\n",
      "tr loss at step 191000 = 1.9771450757980347\n",
      "dev loss = 2.410430908203125\n",
      "lr = 0.015553891783288273\n",
      "--------------------\n",
      "tr loss at step 192000 = 1.977097749710083\n",
      "dev loss = 2.407160758972168\n",
      "lr = 0.015497358112943175\n",
      "--------------------\n",
      "tr loss at step 193000 = 1.9881582260131836\n",
      "dev loss = 2.4272212982177734\n",
      "lr = 0.015441282667736766\n",
      "--------------------\n",
      "tr loss at step 194000 = 1.978845477104187\n",
      "dev loss = 2.4152472019195557\n",
      "lr = 0.015385659529139251\n",
      "--------------------\n",
      "tr loss at step 195000 = 1.9833221435546875\n",
      "dev loss = 2.4271774291992188\n",
      "lr = 0.015330482883990835\n",
      "--------------------\n",
      "tr loss at step 196000 = 1.9806087017059326\n",
      "dev loss = 2.4224722385406494\n",
      "lr = 0.015275747022110046\n",
      "--------------------\n",
      "tr loss at step 197000 = 1.9765514135360718\n",
      "dev loss = 2.410385847091675\n",
      "lr = 0.015221446333968084\n",
      "--------------------\n",
      "tr loss at step 198000 = 1.9774408340454102\n",
      "dev loss = 2.413731098175049\n",
      "lr = 0.015167575308426946\n",
      "--------------------\n",
      "tr loss at step 199000 = 1.9769514799118042\n",
      "dev loss = 2.4163944721221924\n",
      "lr = 0.015114128530539332\n",
      "--------------------\n",
      "tr loss at step 200000 = 1.9812546968460083\n",
      "dev loss = 2.4145936965942383\n",
      "lr = 0.015061100679408272\n"
     ]
    }
   ],
   "source": [
    "for i in range(200000):\n",
    "    batch_ixs = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[batch_ixs]]\n",
    "    h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    reg = 0.05 * sum((p ** 2).mean() for p in parameters)\n",
    "    loss = F.cross_entropy(logits, Ytr[batch_ixs]) + reg\n",
    "\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    loss.backward()\n",
    "    alpha = 0.135\n",
    "    gamma = 1\n",
    "    beta = -np.log(gamma) - 1\n",
    "    lr = np.exp(-((i + 1) ** alpha + beta))\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "\n",
    "    stepi.append(i)\n",
    "    # lri.append(lr)\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(\"--------------------\")\n",
    "        emb = C[Xtr]\n",
    "        h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        overall_loss = F.cross_entropy(logits, Ytr)\n",
    "        print(f\"tr loss at step {i+1} = {overall_loss.item()}\")\n",
    "        emb = C[Xdev]\n",
    "        h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        overall_loss = F.cross_entropy(logits, Ydev)\n",
    "        print(f\"dev loss = {overall_loss.item()}\")\n",
    "        print(f\"lr = {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3fb08b7970>]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9813, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr]\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "overall_loss = F.cross_entropy(logits, Ytr)\n",
    "overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4146, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "overall_loss = F.cross_entropy(logits, Ydev)\n",
    "overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahzailyn.\n",
      "vihaimesi.\n",
      "tate.\n",
      "sacessa.\n",
      "jazonie.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(\"\".join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a15702a29ea01ae7aa6b51d803dd08c0ac5dbedc153ed7a5fc67a3e004dd827d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
